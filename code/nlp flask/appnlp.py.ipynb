{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [03/Jun/2021 19:42:42] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [03/Jun/2021 19:42:42] \"\u001b[33mGET /static/css/style.css HTTP/1.1\u001b[0m\" 404 -\n",
      "C:\\Users\\Tresha\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "127.0.0.1 - - [03/Jun/2021 19:46:57] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [03/Jun/2021 19:46:57] \"\u001b[33mGET /static/css/style.css HTTP/1.1\u001b[0m\" 404 -\n",
      "C:\\Users\\Tresha\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "127.0.0.1 - - [03/Jun/2021 19:47:52] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [03/Jun/2021 19:47:53] \"\u001b[33mGET /static/css/style.css HTTP/1.1\u001b[0m\" 404 -\n",
      "127.0.0.1 - - [03/Jun/2021 19:48:48] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [03/Jun/2021 19:48:48] \"\u001b[33mGET /static/css/style.css HTTP/1.1\u001b[0m\" 404 -\n"
     ]
    }
   ],
   "source": [
    "# importing necessary libraries and functions\n",
    "import numpy as np\n",
    "from flask import Flask, request, jsonify, render_template\n",
    "import pickle\n",
    "import joblib\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from pandas import Series, DataFrame\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "# Import CountVectorizer from feature_extraction.text.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "\n",
    "\n",
    "app = Flask(__name__) #Initialize the flask App\n",
    "modelnlp = pickle.load(open('modelnlp.pkl', 'rb')) # loading the trained model\n",
    "\n",
    "@app.route('/') # Homepage\n",
    "def home():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/predict',methods=['GET','POST'])\n",
    "def predict():\n",
    "    '''\n",
    "    For rendering results on HTML GUI\n",
    "    '''\n",
    "    \n",
    "    train_colorectal = pd.read_csv(\"train_colorectal.csv\")\n",
    "    \n",
    "    y = train_colorectal['Class']\n",
    "    \n",
    "    X = train_colorectal['Text']\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42\n",
    "                                                    )\n",
    "    class LemmaTokenizer:\n",
    "        def __init__(self):\n",
    "            self.wnl = WordNetLemmatizer()\n",
    "        def __call__(self, doc):\n",
    "            return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "    \n",
    "    cvec = CountVectorizer(analyzer='word', tokenizer=LemmaTokenizer(), ngram_range=(1, 1))\n",
    "    \n",
    "    cvec.fit(x_train)\n",
    "    \n",
    "    x_train = cvec.transform(x_train)\n",
    "    \n",
    "    parameters = {'C': [0.001, 0.01, 0.1, 1, 10],\n",
    "              'class_weight': [None, 'balanced'],\n",
    "              'penalty': ['l1', 'l2']}\n",
    "    \n",
    "    lr = LogisticRegression(solver = 'liblinear', \n",
    "                        max_iter = 1000,\n",
    "                        random_state = 42)\n",
    "    modelnlp = GridSearchCV(estimator = lr,                                    # Specify the model we want to GridSearch.\n",
    "                          param_grid = parameters,                           # Specify the grid of parameters we want to search.\n",
    "                          scoring = 'accuracy',                                # Specify recall as the metric to optimize \n",
    "                          cv = 5).fit(x_train, y_train)\n",
    "    \n",
    "    modelnlp.fit(x_train,y_train)\n",
    "    modelnlp.score(x_test,y_test)\n",
    "#Alternative Usage of Saved Model\n",
    "# joblib.dump(clf, 'NB_spam_model.pkl')\n",
    "# NB_spam_model = open('NB_spam_model.pkl','rb')\n",
    "# clf = joblib.load(NB_spam_model)\n",
    "\n",
    "    if request.method == 'POST':\n",
    "        message = request.form['message']\n",
    "    data = [message]\n",
    "    vect = cv.transform(data).toarray()\n",
    "    my_prediction = modelnlp.predict(vect)\n",
    "    return render_template('result.html',prediction = my_prediction)\n",
    "    \n",
    "    # retrieving values from form\n",
    "    #init_features = [str(x) for x in request.form.values()]\n",
    "    #final_features = [np.array(init_features)]\n",
    "\n",
    "    #prediction = model.predict(final_features) # making prediction\n",
    "\n",
    "\n",
    "    return render_template('index.html', prediction_text='Predicted Class: {}'.format(prediction)) # rendering the predicted result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
